{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae0e8ff4-beab-4407-ac62-b7b5c0c197c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Phase Four: Experiment, Fine-tune, and Optimal Pipeline\n",
    "\n",
    "Section 5 Team 3-1\n",
    "\n",
    "Team members: \n",
    "  - A Adam Saleh\n",
    "  - Liang Li\n",
    "  - Qian Qiao\n",
    "  - Shuo Wang \n",
    "  \n",
    "\n",
    "Notebook link: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836275473/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa83453-49ba-4103-bb11-145ed19cabd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Phase Leader Plan \n",
    "| Project Phase                                       | Phase Leader                | Email | Leader Photo |\n",
    "|-----------------------------------------------------|-----------------------------|-------|--------------|\n",
    "| 1 - Project plan                                    | Qiao Qian                   | qianqiao@ischool.berkeley.edu |    üòä      |\n",
    "| 2 - EDA                                            | Shuo Wang               | swang@ischool.berkeley.edu |    üòÅ     |\n",
    "| 3 - Feature engineering                           | Liang Li                   | liliang@ischool.berkeley.edu|       üòÑ       |\n",
    "| 4 - **Optimal algorithm**                              | **Adam Saleh**                  | **a.adam.saleh@ischool.berkeley.edu** |    üòÑ        |\n",
    "| 5 - Final Report and In-class Presentation          | Shared - Final Presentation |       |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47b24315-fa38-4a7e-9fb1-9a3d24879db6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Credit Assignment Plan\n",
    "\n",
    "| Name         | Credit Assignment | Time Spent |\n",
    "|--------------|-------------------| -- |\n",
    "| Shuo Wang    | Abstract, EDA, Downsample experiment | 15 hours |\n",
    "| Liang Li     | Final Model Results, Feature Importance, Gap Analysis, Conclusion   | 15 hours |\n",
    "| A Adam Saleh | Neural Network, Novel Approach, Final Solution & Interpretation   | 15 hours |\n",
    "| Qian Qiao    | Data leakage, Feature enginnering, experiment plotting | 20 hours |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a2a61c-8d28-4c1e-9547-85769a2c6e12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Project Abstract - Phase Four\n",
    "\n",
    "This project aims to develop a predictive model for estimating flight delays using historical airline and weather data from 2015 to 2019. Flight delays significantly impact travel planning and operational costs for airlines and airports. The predictive model addresses this issue by empowering customers to plan their journeys efficiently and reducing costs for aviation stakeholders. The project involves various phases, including data preprocessing, exploratory data analysis (EDA), feature engineering, model selection, hyperparameter tuning, and performance evaluation.\n",
    "\n",
    "During the EDA phase, data from different time periods are examined, and strategies for data ingestion, imputation, cross-validation, and model selection are determined. Feature engineering involves handling missing values, transforming numerical features, creating new relevant features, and encoding categorical features. Different machine learning algorithms, including Logistic Regression, Random Forest, XGBoost, and Neural Networks, are experimented with to identify the best model for predicting flight delays. For metrics, the models were evaluated using F beta metrics (while also report F1 score, accuracy, precision, recall, and AUC metrics). \n",
    "\n",
    "The baseline logistic regression model, trained on a 3-month dataset, got an F1 score of 0.758 and a F2 score of 0.961 (Note: this F2 is for label 0; Weighted F2 score was not computed for the base case as its computation was implemented in later stages of the project). The accuracy is 0.834, precision is 0.695, recall is 0.834, and the ROC AUC is 0.665.  \n",
    "\n",
    "The final Random Forest model, trained on a 5-year dataset (2015-2019), demonstrates promising results. It predicts that 18% of flights will experience delays, achieving an F1 score of 0.731 and a weighted F2 score of 0.784. The model's accuracy is 0.814, with precision and recall values of 0.848 and 0.814, respectively. The Receiver Operating Characteristic Area Under Curve (ROC AUC) stands at 0.701.\n",
    "\n",
    "Key features influencing flight delays include departure time, arrival time, hour of the day, season of the year, humidity, taxiing duration, precipitation, temperature, sea level pressure, and distance. The model's predictive abilities can be enhanced further by collecting and considering weather related features for the arrival airport, exploring other advanced algorithms, and optimizing PySpark code for improved efficiency.\n",
    "\n",
    "For execution time, the models are running more than 1 hours for 3-month data and more than 10 hours for 5-year data. This is the challenge where model needs to be further improved perhaps by using a more creative partiioning approach. \n",
    "\n",
    "In conclusion, the developed predictive model provides valuable insights into flight delays, enabling passengers to plan better and benefiting the aviation industry by minimizing operational costs and improving passenger satisfaction. The project demonstrates effective strategies for data preprocessing, feature engineering, model selection, and performance evaluation, and suggests avenues for future improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a0464b-ae0e-4f49-9775-6d5945909c4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5440e51-c36a-4054-b7f1-cf9331d423e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataset Introduction\n",
    "There are four datasets used for this project. the first dataset (**Flights Data**) is for flights originating and landing from/to US Domestic airports from 2015 to 2021. the second dataset  (**Weather Data**) is for weather information from 2015 to 2021. these two datasets are big tables with tens to hundreds of millions of rows. the third dataset (**Station Data**) is for the weather stations, which contains location and proximity information to help identify the closest weather station to any airport in the first dataset. the last dataset (**Airport Code Data**) is for IATA airport code, a three-letter code that is used in passenger reservation, ticketing, and baggage-handling systems. the last dataset helps to connect airport to stations and, subsequently, to weather data. the last two tables are relatively smaller tables with tens of thousands of rows. the flight table is expected to be joined with other three tables to obtain useful weather related features pertinenet to each of the flight records.  \n",
    "Data dictionary: https://www.transtats.bts.gov/Fields.asp?gnoyr_VQ=FGJ\n",
    "\n",
    "There are joined tables (OTPW) using data from all previously mentioned data tables (Airline, weather, sation & AITA). OTPW tables for different time periods are also provided. These consist of 3m, 6m, 1yr and the complete dataset (2015-2019). The total size of the three month OTPW dataset is 1,500,620,247 bytes, with 1,401,363 rows and 216 columns. The total size of the 2015-2019 OTPW dataset is 6,525,616,408 bytes, with 31,673,119 rows and 214 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c138c35-df32-4bf1-a224-735edb1d52b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Summarize EDA and Feature Engineering \n",
    "\n",
    "In previous phases, we transfer dataframe type from csv to parquet, drop columns with 90%+ missing values, remove columns with constant value, update columns types based on analysis, removing columns representing similar information or not relevant to target value, do feature transformations, add graph-based features, and introduce novel predictive attributes, such as holidays, hour of the day, and season of the year.\n",
    "\n",
    "We further explore the families' features such as time-related features including day of the week, month, year. This help capture trends and seasonality in flight delays. We also expand this to include holidays check its affect on delays. The seasonality analysis plots can be found in this section.  \n",
    "\n",
    "The details of all the feature engineering processes mentioned in this report can be found in the <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/411545566820969/command/3984215836263118\" target=\"_blank\">Link of EDA abd Feature Engineering Notebook</a> \n",
    "\n",
    "In Phase IV, we do further feature engineering as follows: \n",
    "1. We further consider the effect of delay in previous airport on departing delay prediction on current airport. A new column \"DEL_W2HR\" is created in the dataframe, in this column, if the flight departure time difference between previous airport and current airport is longer than 2 hours and the departure time in previous airport is delayed, then the label value is 1, otherwise, the label value is 0. The detailed process can be found in <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836277067/command/4444148632303226\" target=\"_blank\">2-hours Requirement</a>. The detailed cross validation, modeling, prediction, and evaluation can be found in <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632303801/command/4444148632303802\" target=\"_blank\">Case 15</a> \n",
    "\n",
    "2. We apply \"downsample method\" to 3 month data to make our data balanced (delay vs non-delay). We downsample non-delay data in each week to be balanced with delay flights and do the prediction and evaluation. Please refer to case 2b for more details.  \n",
    "<a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632305377/command/4444148632305378\" target=\"_blank\">Case 2b: downsample method</a>  \n",
    "\n",
    "3. We remove categorical nominal columns \"ORIGIN\" and \"DEST\". These two columns has 311 and 315 unique values respectively. When we do one-hot encoding for them, it will increase computational analysis time. Considering the scalability concerns, we decide to remove them.  \n",
    "\n",
    "4. For numerical missing value imputation, instead of using the mean value of the columns acrosing whole dataset, we will sort the column values based on time and will replace the missing value with the mean value of its previous values.\n",
    "\n",
    "The summary for EDA and feature engineering can be found in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcbd8ff9-11d1-4780-8904-ee41b3338dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Families Features - Seasonality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0717cea4-6a13-4f95-a71a-0075b64834ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Holiday Trend\n",
    "While the model inherently captures some level of seasonality, we can further enhance its ability to detect seasonal variations by explicitly adding holidays and other time-based features as additional predictors. Utilizing a data source from Kaggle.com, we integrated all U.S. holidays into our main dataset and the 60-month dataset. Upon observation, we noticed that the percentage of flight delays varied significantly across different holidays. therefore, we believe that incorporating holidays as a new feature would add value to our modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77fbc667-f21c-4d57-a630-048487e45cf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "<img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/holidays.png?raw=true\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f41e7a21-d7dc-4307-8138-ff2973139f15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Daily Trend\n",
    "Flight delays also vary with the time of day. Our EDA revealed that the percentage of delays is relatively low during the early morning hours from 5 am to 8 am. However, as the day progresses, the percentage of delays increases, reaching its peak in the evening from 8 pm to 9 pm. Understanding this daily pattern can provide our model with valuable insights into flight delay trends throughout the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82687c15-1dbb-4c69-a28a-b60bf75736d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/hours.png?raw=true\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba214a6-0260-4f7c-9477-b717528da8d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Seasonality \n",
    "\n",
    "Zooming in the timeframe, we examined the relationship between flight delays and months, quarters, or seasons. We observed significant differences by season. For instance, in the fall, there was a higher percentage of flight delays compared to other seasons, while spring had the lowest percentage of delays based on the 60-month data. As a result of this observation, we have decided to incorporate a new feature into our model that takes into account the seasonality of flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "029ce5be-f3fa-46f4-89fc-1ed1f940612d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/seasons.png?raw=true\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "251a5222-7192-40b3-9478-cd7c1a743dae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### EDA and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab554ff-e878-4dda-a1cd-534dbad77341",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Feature Selection and Transformation Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86981bd5-bc55-4dab-a765-64e955aec73b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"7b4dc5c6-9f7a-44cd-96ed-97258a12ab3e\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7b4dc5c6-9f7a-44cd-96ed-97258a12ab3e\")) {                    Plotly.newPlot(                        \"7b4dc5c6-9f7a-44cd-96ed-97258a12ab3e\",                        [{\"x\":[214,122,121,46,30,42,46],\"y\":[\"60M OTPW Dataset\",\"Removing Columns with 90% missing data\",\"Removing constant value columns\",\"Removing columns based on EDA analysis\",\"Transformation of categorical features\",\"Additon of square terms and log terms \",\"Addition of time-based/Graph-based new features\"],\"type\":\"funnel\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    )                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funnel plot for the features selections\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "fig = go.Figure(go.Funnel(\n",
    "    y = [\"60M OTPW Dataset\", \"Removing Columns with 90% missing data\", \"Removing constant value columns\", \"Removing columns based on EDA analysis\",\"Transformation of categorical features\" ,\"Additon of square terms and log terms \",\"Addition of time-based/Graph-based new features\"],\n",
    "    x = [214, 122, 121, 46, 30,42, 46 ]))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6183bf4-9076-4b43-9c45-7fb5ba6cbc26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Final Feature Selection\n",
    "\n",
    "Below is a table showing the final selection of the features to be used as the base for model experimentaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c0e9d94-6ca0-498c-8d7b-6624d7ccfd04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Numerical features list\n",
    "\n",
    "| Data Source      | Column Name                                                                                                                                                                                                                                    | Data Type    | Description                                                                               |\n",
    "|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|-----------------------------------------------------------------------------------------|\n",
    "| Flight           | 1.DEP_TIME, 2.ARR_TIME, 3.AIR_TIME, 4.TAXI_OUT, 5.TAXI_IN                                                                                                                                                                                      | Numeric      | Provide the flight time information                                                         |\n",
    "| Flight           | 6.DISTANCE, 7.ELEVATION                                                                                                                                                                                                                        | Numeric      | the distance of the flight and elevation of the airport may impact the delays          |\n",
    "| Weather          | 8.origin_station_dis, 9.dest_station_dis:                                                                                                                                                                                                          | Numeric      |                                                                                         |\n",
    "| Weather          | 10.HourlyAltimeterSetting, 11.HourlyDewPointTemperature, 12.HourlyDryBulbTemperature, 13.HourlyPrecipitation, 14.HourlyRelativeHumidity, 15.HourlySeaLevelPressure, 16.HourlyStationPressure,  17.HourlyWetBulbTemperature, 18.HourlyWindSpeed | Numeric      | Weather features that may impact the delays                                             |\n",
    "| Created Features | 19.TAXI_OUT_sq, 20.TAXI_IN_sq, 21.DISTANCE_sq, 22.ELEVATION_sq, 23.HourlyPrecipitation_sq, 24.HourlyWindSpeed_sq , 25.TAXI_OUT_log, 26.TAXI_IN_log, 27.DISTANCE_log, 28.ELEVATION_log, 29.HourlyPrecipitation_log, 30.HourlyWindSpeed_log                                                                                                                              | Numeric      | Data transformation to capture the non-linear relationships between features and target |\n",
    "| Created Features | 31.Out_Degree                                                                                                                             | Numeric      | Graph feature represents the degree centrality of each airport on a daily basis. |\n",
    "| Created Features | 32.HourlyVisibility_new                                                                                                                           | Numeric      | Cast the numeric value from string to represents the level of visibility |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ac07f5-87a5-4fc4-ad1e-7a8fddcd7ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Categorical features list - ordinal\n",
    "\n",
    "| Data Source      | Column Name                                                                                                                                                                                                                                    | Data Type    | Despcription                                                                               |\n",
    "|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|-----------------------------------------------------------------------------------------|\n",
    "| Flights          | 1.MONTH, 2.DAY_OF_MONTH, 3.DAY_OF_WEEK                                                                                                                                                         | Categorical  |                                                                                         |\n",
    "| Weather          | 4.origin_type, 5.dest_type,                                                                                                                                                                                  | Categorical  |  the size of the airport represents its capacity to handle travelers and can significantly impact flight delays.                                                                                       |\n",
    "| Created Features | 6.WEEK                                                                                                                                                                                                                                       | Categorical  | Apply for the cross validation split                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9ecfa0-f25e-4adf-969f-e8a7f89fc73c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Categorical features list - nominal\n",
    "\n",
    "| Data Source      | Column Name                                                                                                                                                                                                                                    | Data Type    | Despcription                                                                               |\n",
    "|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------|-----------------------------------------------------------------------------------------|\n",
    "| Flights          | 7.OP_CARRIER_AIRLINE_ID                                                                                                                                                        | Categorical  |                                                                                         |\n",
    "| Flight          | 8.DIVERTED,                                                                                                                                                                                  | Categorical  |                                                                                         |\n",
    "| Weather          | 9.HourlySkyConditions, 10.HourlyWindDirection                                                                                                                                                                            | Categorical  |     Transformation for enhancing the indexing and encoding efficiency                                                                                    |           \n",
    "| Created Features | 11.holiday, 12.Delay_Daily_Prob, 13.Delay_Season_Prob                                                                                                                                                                                                                                       | Categorical  | Time-based features capture the seasonality                                                   |\n",
    "| Created Features | 14.DEL_W2HR | Categorical  | The delay in previous airport will affect departure time in current airport |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff38cbb-3373-48f7-b70d-a2fbeb35ca7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88307970-96bd-4b85-9a17-72ad5edd2800",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A classifier trainer based on the Multilayer Perceptron estimator in PySPark was implemented using the 60-months data. Two different MLP Network architectures were used:\n",
    "\n",
    "1. [input_size, hidden_layer_size_1, output_size] = [66, 30, 2]\n",
    "2. [input_size, hidden_layer_size_1, hidden_layer_size_2, output_size] = [66, 40, 20, 2]\n",
    "\n",
    "Where number of inputs is equal to the size of the feature vectors and the number of outputs is equal to the total number of labels (2 in this case). the two neural network architectures differe in the number of hidden layers (used only 1 and 2 for this expriment) and in the number of nodes per hidden layer. the number of nodes per hiddem layer was taken as the average between input size and output size ([input_size+output_size]/2).\n",
    "\n",
    "The cross validation folds used for this model were the same as those developed in Case 13. Initial attempts (Case 14) were not successful and errors seem to indicate that it could be a memory issue. Our second attempt (Case 14) was to use 10% of the 60M data. Although the code seems to yield accurate reults and metrics for the first CV fold (see Table below), the program kept failing with no apparent reason after completing the first fold. Suspecting it is a memory issue, we implemented the same procedure but on the smaller 3M_NC data with again 10% of the data and 5 CV folds (Case 14b). This time the program ran and gave metrics for the first two folds (see Table below) before it failed again. Our third and final attempt (Case 14c) was to remove all the hot encoding feature in an attempt to reduce the feature space. This time, the model provided the results for the first NN architecture but failed in the second architecture (see table below).\n",
    "\n",
    "We believe that the experiments we ran points to potential memory issue with this model. It should be noted that other attempts were made: (1) remove the hidden layers, and (2) avoid the CV step and test directly. However, the problem persisted. As such we teminated the efforts in the interest of time.\n",
    "\n",
    "Links to the expriments:  \n",
    "* Case 14: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836272811/command/3984215836272857  \n",
    "* Case 14b: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632303329/command/4444148632303379  \n",
    "* Case 14c: https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632303434/command/4444148632303484"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c559413-16b6-4e7b-8bc8-bda0aeeefc36",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### NN-MLP Results Summary \n",
    "\n",
    "<img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/image%20(7).png?raw=true\" width=1500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "435ffff4-89e0-4395-96fe-44f3edc740f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46dad61-56f2-4a97-bed1-98aa587a5241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is data leakage \n",
    "\n",
    "Data leakage occurs when information from outside the training dataset is improperly used to create or evaluate a machine learning model, leading to overly optimistic or inaccurate performance metrics. It happens when the model learns patterns that won't generalize to new, unseen data, and can result in models that appear highly accurate but fail to perform well in real-world scenarios. Leakage can arise from various sources, including feature engineering, target leakage, and temporal leakage.\n",
    "\n",
    "For this flight delay prediction project, we have access to historical flight data that includes information about whether a flight was delayed or not. Additionaly, we have weather data for each flight's departure date and location. the data leakage may happen in these scenarios: \n",
    "1. Feature engineering leakage: there is a feature `HourlyPrecipitation` which represents the amount of precipitation at a specific hour nearby the airport. the data leakage arises if the actual HourlyPrecipitation value for the entire day (including future hours) to predict flight delays, you would effectively be using information from the future to make predictions. This can lead to an overly optimistic performance on your model during training and validation since it's essentially \"cheating\" by using information that wouldn't be available in a real-world scenario.\n",
    "2. Imputation leakage: When we decide to imputate the missing `HourlyPrecipitation` values using the mean precipitation for the entire day. the issue here is that using the mean precipitation for the entire day to impute missing values for individual hours can lead to data leakage. the mean precipitation for the entire day includes information about future hours that would not be known at the time of making predictions. \n",
    "3. Temporal split: Using recent years' data for training and older years' data for testing introduces temporal leakage. the model learns patterns from the past to predict the future, which is not realistic. Thus this flight delay prediction project cannot use the random split for train, validation and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51ce4df-1928-4232-b894-b99db8322dbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Techniques to avoid leakage \n",
    "\n",
    "To address potential data leakage in the aforementioned scenarios, we have implemented various techniques to mitigate these issues within the flight delay prediction project.\n",
    "\n",
    "1. Feature engineering considerations: We have leveraged lag and window functions to generate a new feature named `DEL_W2HR` This feature indicates whether a plane has any delayed flight two hours before its departure time. Specifically, flights that were delayed more than two hours in advance are labeled as \"1\", signifying their potential use in predictions. In contrast, if a flight was delayed within the two-hour window or experienced no delays, it is marked as \"0\".\n",
    "2. Backfill imputation: We have taken measures to ensure that missing values are filled appropriately. Rather than using the mean of all available data for imputation, we only consider information before the point of missing value. This approach minimizes the risk of introducing data leakage during the imputation process. \n",
    "3. Rolling window Cross-Validation stratey: \n",
    "  - For the 2015 3-month data, we performed a train/validation/test split using weeks, where weeks 1 to 11 are used for training, and week 12 is used as the test set. To create cross-validation folds, we employed a rolling window split, with the first fold including weeks 1-6 as the training set and week 7 as the validation set. Subsequent folds slide by one week for both training and validation sets.\n",
    "\n",
    "  - For the 2015 to 2019 5-year data, we used the entire years as training data, with 2019 designated as the test set. Similar to the previous case, we utilized the rolling window approach for cross-validation folds. the first fold comprises data from 2015 as the training set and 2016 as the validation set. the second fold includes data from 2015 and 2016 as the training set and data from 2017 as the validation set. This process continues, with each fold adding one year of data to the training set and using the next year as the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd37018-b2b8-48df-bf65-aa03406e45a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Known Data Leakge Potential \n",
    "\n",
    "One of our newly created features, Delay_Daily_Prob, was created by grouping the entire data set on hourly departure time (see details in Phase III report). A probability distribution function expressing the percent delay was then created for each hour of the day and applied to each record, thus, creating a new seasonality related feature. We realize now that the approach might have created a pathway for data leakage since the percent of delay (the probability basis) for each hour was calculated based on the entire group data for each hour. In future runs, we will make this feature engineering step as part of the pipeline whereby we develop the new feature using teh same technique by grouping on the hour, again, but this time the grouping will be conducted only on the training data. We can then, for each record in the test data, create the same feature but using the probability distribution obtained from the training data.\n",
    "\n",
    "The same applies to the second similar seasonal feature we created, Delay_Season_Prob. This feature was created again, by grouping on seasons for the entire data. In future runs, the creation of this feature will also be made a part of the pipleine whereby the probability for each season will be based on grouping of season records in the train data only and applied to both train and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48484873-0b99-4a79-ae0e-79399586cfc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7047df6-3a6c-446b-91ed-57ecc234113a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The model pipeline consists of several key stages aimed at preparing the data and training the logistic regression model for flight delay prediction:\n",
    "\n",
    "1. Data Pre-processing: Categorical features are indexed and one-hot encoded to transform them into numerical representations. Missing values in features are imputed to ensure complete data.\n",
    "\n",
    "2. Data Staging: The pre-processed features are combined into a single feature vector. The target variable \"DEP_DEL15\" is set as a binary indicator, setting the stage for model training.\n",
    "\n",
    "3. Scaling Numeric Features: Numeric features are scaled using the StandardScaler. This process centers the features around 0 and adjusts their scale, making them more comparable and facilitating model training.\n",
    "\n",
    "4. Logistic Regression Estimator: The logistic regression estimator is employed for the binary classification task. An Elastic Net regularization approach is used, with elasticNetParam set to 0.5. This balances L1 and L2 regularization to prevent overfitting and enhance generalization.\n",
    "\n",
    "5. Pipeline Assembly: The different components, including pre-processing, scaling, and the logistic regression estimator, are assembled into a single pipeline for streamlined execution.\n",
    "\n",
    "6. Train/Test Splitting: The data is divided into training and testing datasets based on years. Year 2015 to 2018 are designated as the training set, and year 2019 is used as the test set. \n",
    "\n",
    "7. Cross-Validation: The training data is divided into cross-validation folds for hyperparameter tuning. This approach occurs before any data transformation or model training, ensuring avoidance of data leakage.\n",
    "\n",
    "8. Hyperparameter Tuning: Cross-validation is used to fine-tune the model's hyperparameters, helping identify the best configuration for optimal performance.\n",
    "\n",
    "9. Model Training: The model is trained using the training dataset, taking into account the selected hyperparameters.\n",
    "\n",
    "10. Model Saving: The trained model is saved for future use and evaluation. It is then applied to the testing dataset to assess its performance using various evaluation metrics.\n",
    "\n",
    "11. Model Evaluation: The saved model is loaded and applied to the testing dataset to evaluate its performance. Different evaluation metrics are used to assess the model's F2, F1, accuracy, precision, recall, and ROC_AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c35ad94e-f4d0-46fc-a1f0-8e92b418b902",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Final Model Pipeline For 60M Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe7bf1a-85c1-4561-92cd-bbeb4ac337d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/pipeline.png?raw=true\" width=1500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd7e293-54a2-40d6-a698-482b6201e9f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## List of Hyperparameters\n",
    "\n",
    "Considering the execution time constraints, we have limited the number of hyperparameters for tuning. You can find more details for the parameter setting for each case in the experiments table below.  \n",
    "\n",
    "- Logistic Regression: \n",
    "  - regularization coefficient\n",
    "\n",
    "- Random Forest: \n",
    "  - number of trees\n",
    "  - maximum depth of tree\n",
    "\n",
    "- XGBoost:  \n",
    "  - number of trees \n",
    "  - maximum depth of tree\n",
    "  - learning rate\n",
    "  \n",
    "- Neural Network: \n",
    "  - size of hidden layer\n",
    "  - number of hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27193676-c6dd-4c68-aa0a-46a591cc2732",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Evaluation Metrics\n",
    "The choice of evaluation metrics is crucial to measure the effectiveness and impact of the predictive model. The business case revolves around enabling customers to plan their journeys more efficiently and minimizing operational costs for airlines and airports. In this project, we chose the following metrics to evaluate the model performance.\n",
    "\n",
    "- F Beta Score (Beta = 2.0):\n",
    "F Beta score with a higher beta value (such as 2.0) places more emphasis on recall. This aligns with the business objective of minimizing false negatives, i.e., correctly identifying flights that might be delayed (positive class) to ensure customers can plan accordingly.\n",
    "The higher beta value is particularly useful in cases where the cost of missing a positive prediction (flight delay) is high compared to the cost of false alarms (incorrectly predicting a delay).\n",
    "This metric is valuable for achieving a balance between precision and recall and addresses the business case's goal of efficient journey planning and minimizing disruptions.  \n",
    "\n",
    "- F1 Score:\n",
    "F1 score is the harmonic mean of precision and recall. It provides a balanced view of a model's overall performance.\n",
    "It is especially relevant when both false positives and false negatives have similar consequences. In this case, it helps find a compromise between minimizing both types of errors.\n",
    "For the project, F1 score can indicate how well the model balances the accurate identification of delays with avoiding unnecessary warnings.  \n",
    "\n",
    "- Accuracy:\n",
    "Accuracy measures the proportion of correct predictions among all predictions.\n",
    "While accuracy is a common metric, it might not be the best fit for imbalanced datasets, as small classes (in this case, flight delays) might be overshadowed by larger ones.\n",
    "However, it still provides a general measure of the model's overall correctness.  \n",
    "\n",
    "- Precision and Recall:\n",
    "Precision and recall are fundamental metrics when dealing with imbalanced datasets and when the cost of false positives and false negatives is not uniform.\n",
    "High precision ensures that when a delay is predicted, it's likely to be accurate, minimizing unnecessary disruptions for passengers.\n",
    "High recall ensures that most of the actual delays are identified, reducing the risk of passengers missing their flights due to inaccurate predictions.  \n",
    "  \n",
    "- Area Under the ROC Curve (AUC-ROC):\n",
    "AUC-ROC measures the model's ability to distinguish between positive and negative instances across different probability thresholds.\n",
    "In the context of the project, AUC-ROC assesses the model's ability to rank flights by their likelihood of being delayed, which can be crucial for prioritizing alerts and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6ff22d-c993-4e01-a31c-0935a7f2c64f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loss Function and Regularization \n",
    "\n",
    "### Loss Function \n",
    "Since flight delay prediction is a binary classification problem, we decide to use binary cross-entropy loss (log loss) as loss function. This is the most common loss function for logistic regression and typically be used for XGBoost and Neural Network. \n",
    "\n",
    "<img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/loss%20.png?raw=true\" width=500>\n",
    "\n",
    "Where:\n",
    "\n",
    "- N is the number of samples in your dataset.\n",
    "- y _i is the true label of the i-th sample (0 or 1).\n",
    "- p_i is the predicted probability that the i-th sample belongs to the positive class (0 to 1).\n",
    "\n",
    "In Case 2, we examined the training loss and validation loss values across various parameters. The plot illustrates that the parameter value of 0.01 corresponds to the lowest values for both training and validation losses, suggesting that 0.01 is the optimal parameter for our baseline. This finding aligns with the outcomes of hyperparameter tuning, which were determined based on the F2 score. Although it's acknowledged that exploring additional hyperparameters or conducting a grid search could potentially uncover more suitable values, we opted to restrict our search due to constraints on execution time. As we address the execution time challenges, we remain open to further enhancing our hyperparameter optimization in future iterations of our project.\n",
    "\n",
    "<img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/Picture1.png?raw=true\" width=800>\n",
    "\n",
    "### Regularization \n",
    "\n",
    "We set regParam = 0.5 for the regularization, which means that we are applying an equal combination of L1 (Ridge) and L2 (Lasso) regularization to our model to help it generalize better on unseen data and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78cf2c6b-bcdb-4bef-93fa-2f22e3cc79b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Scalability Issues\n",
    "\n",
    "We faced scalability challenges primarily due to the sheer volume of data and the complexity of our models. For instance, using the 3-month dataset as a baseline, our pipeline's execution time was around 1.46 hours. However, as we scaled up to the 60-month dataset, the execution time increased significantly to approximately 9.62 hours. The experiment results in the table in th enext section provide a clear overview of the execution times for each scenario.\n",
    "\n",
    "To tackle these scalability issues and enhance efficiency, we implemented several strategies. Firstly, we recognized that running multiple models simultaneously could lead to resource competition and reduced efficiency. Consequently, we optimized the pipeline by running models sequentially, which improved overall resource utilization.\n",
    "\n",
    "Furthermore, we experimented with different partition settings for our cluster. Initially, our setting restricted the cluster to a single worker, which affected efficiency. By reverting to the default partition configuration and regaining access to eight workers, we significantly improved efficiency and decreased execution times.\n",
    "\n",
    "Additionally, we introduced caching at key points in the pipeline. For example, we cached the train and validation fold repartition within the cross-validation loop. While this optimization contributed to efficiency improvements, the impact was not as substantial as initially anticipated.\n",
    "\n",
    "Despite our efforts, we continued to grapple with execution time challenges. We experimented with feature reduction and parameter simplification to expedite the pipeline. However, we were unable to find a definitive solution to effectively mitigate the execution time issue.\n",
    "\n",
    "Our journey to address scalability challenges involved optimizing model execution sequencing, adjusting cluster partition settings, employing caching strategies, and exploring feature and parameter modifications. While we made considerable progress, further improvements are necessary to fully resolve the execution time concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd0c2870-3cc8-4683-be3a-cba9a2bf0cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Final Solution & Novel Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed10463c-04ca-44a3-968f-86d11bf82dfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Final Solution\n",
    "\n",
    "Our final model is reprsented by [Case 15](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632303801/command/4444148632303802).  In this final case, we implemented the following conditions:\n",
    "* Dataset: 60M_NC data\n",
    "* Same features as in [Case 6](https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836266900/command/3984215836267168) (see table below) + the new 2-hour tracking feature (`\"DEL_W2HR\"`)\n",
    "* Estimator: Random Forest with best hyperparamters established based on Case 6 (3M_NC data). These pramaters are:  \n",
    "  * `best_param_numTrees = 50`  \n",
    "  * `best_param_maxDepth = 5` instead of 20 to avoid overfittting\n",
    "\n",
    "For the final test results, we also computed an F2 weighted score as follows:   \n",
    "```\n",
    "f2_weighted = (f2_1 * num_1 + f2_0 * num_0)/(num_1+num_0)\n",
    "```\n",
    "\n",
    "where\n",
    "```\n",
    "  f2_1 = (1+beta^2) * (precision_1 * recall_1)/((beta^2 * precision_1)+recall_1)  \n",
    "  f2_0 = (1+beta^2) * (precision_0 * recall_0)/((beta^2 * precision_0)+recall_0)  \n",
    "  num_1 = number of records with delays   \n",
    "  num_0 = number of records with no delays\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d079cbfd-455d-4161-8b88-837c4595f604",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Novel Approach\n",
    "\n",
    "Our novel approach consisted of implementing an ensemble method using the following scheme:  \n",
    "* Implement a NN MLP model using multiple nueral netwrok architectures on the 60M data\n",
    "* Choose best architecture and make prediction on the entire data set (train+test)\n",
    "* Use the prediction results as an additional feature that we add to the 60M dataset\n",
    "* Save the resulting 60M dataset with the new feature \n",
    "* Proceed with the final RF model using the new dataset to make the final prediction\n",
    "\n",
    "This ensemble method is thought to improve our prediciotn. However, the memory issues we faced in the neural netwrok implementation discussed earlier in this report prevented us from completing this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e7a685-0072-4974-9cad-275b01048f82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Results & Discussion\n",
    "\n",
    "### Experiments\n",
    "\n",
    "We conducted 15 experiments to assess different conditions of the flight dealy prediciton problem. These conditions and modeling details are provided in the table below. The purpose of these experiments is to identify the best combination of features, hyperparameters and modeling conditions that can result in the most accurate prediction of flight delays. The design of the experiments involved comparing multiple cases, each with different combinations of hyperparameters and features, and evaluating their performance using various metrics. Most experiments were performed on the smaller dataset (3-month) for expediency and using the simple logistic regression estimator used in the baseline model.\n",
    "\n",
    "Notes\n",
    "* In the \"Data\" column, `3M` denotes the 3 month data in 2015 with both cancelled flights and non-cancelled flights, and cancelled flights are treated as delayed. `3M_NC` dentoes the 3 month data with non-cancelled flights only.\n",
    "* In the \"Estimator\" column, `LR` denotes Logistic Regression model, `RF` denotes Random Forest model, `XGB` denotes XGBoot model, and `NN` denotes Neural Network model.\n",
    "* In the \"CV\" column, `CV` denotes cross-validation, `CV=5; 6-week rolling time folds` denotes there are 5 folds, where each fold spans across 6 weeks in a rolling window fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd101e8-880b-459b-8ec3-07103f2baf80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter Tuning \n",
    "There are four models explored in this project, inlcuding Logistic Regression, Random Forest, XGBoost, and Neural Network. A number of hyperparameters were tuned for each model associated with the cases described above. the selected hyperparameters for tuning  were as followss:\n",
    "- Logistic Regression: regularization coefficient; e.g., `regParam = [0.01, 0.1, 1]`\n",
    "- Random Forest: number of tree, maximum depth of tree; e.g., `numTreesList = [20, 50, 100], maxDepthList = [5, 10, 15, 20]`\n",
    "- XGBoost: number of trees, maximum depth of tree, learning rate; e.g., `n_estimators_list = [100, 200, 500], max_depth_list = [6, 8, 10], eta_list = [0.01, 0.1, 0.3]`\n",
    "- MLP (Neural Network): e.g., `[input_size, hidden_layer_size_1, hidden_layer_size_2, output_size] = [66, 40, 20, 2]`\n",
    "\n",
    "The hyperparameters were tuned based on the best score using the F2 (F score with beta=2) metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0d1241-9466-4103-9562-d699cf6884e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "| No. | Purpose                                                                                                            | Data       | # of Features | Estimator | HyperParameters                                    | CV                              | Cases  | Exec TIme                  | Eval Metrics |\n",
    "|:---:|--------------------------------------------------------------------------------------------------------------------|------------|---------------|-----------|---------------------------------------|---------------------------------|-------|------------------|----------|\n",
    "|  1  | Check effect of canceled flights - Case1 with canceled flight records (3M). For Case1 we assumed that the cancelation was due to excessive delays. As such , we imputed the DEP_DEL15 with '1' indicating a delay.                          | 3M & 3M_NC | 36            | LR        | regParam                              | CV=5; 6-week rolling time folds | 1 2   | 1.42hrs              |    F2(lbl_0)=0.9596 <br>f1 score=0.7476 <br>areaUnderROC=0.6619|\n",
    "|    | Case2 without the records(3M_NC)   | 3M & 3M_NC | 36            | LR        | regParam                              | CV=5; 6-week rolling time folds | 1 2   | 1.47hrs              |    F2 (lbl_0)= 0.9616 <br> f1=0.7582 <br> areaUnderROC=0.6648  |\n",
    "|  2  | Check effects of Holidays - Case3 with holidays compared with previous Case2 without (case 2 was used becuase results from above expriment shows using 3M_NC yield better results. therefore all subsequent cases use 3M_NC)                                        | 3M_NC      | 37            | LR        | regParam                              | CV=5; 6-week rolling time folds | 3 2   | 1.44 hrs              |   F2 (lbl_0)=0.9616 <br> f1 score=0.75815 <br>areaUnderROC=0.6648      |\n",
    "|  3  | Check effects of daily seasonality - Case4 with new ‚ÄòDelay_Daily_Prob‚Äô feature acompared to Case2 without      | 3M_NC      | 37            | LR        | regParam                              | Case 5; 6-week rolling time folds | 4 2   | 1.46 hrs              |   F2(lbl_0)=0.9616 <br> f1 score: 0.75815 <br> areaUnderROC=0.6648|\n",
    "|  4  | Check effects of season seasonality - Case5 with new ‚ÄòDelay_Season_Prob‚Äô feature compared to Case2 without    | 3M_NC      | 37            | LR        | regParam                              | CV=5; 6-week rolling time folds | 5 2   | 1.41 hrs              |    F2 (lbl_0)=0.9616 <br> f1 score=0.75815 <br> areaUnderROC=0.6648   |\n",
    "|  5  | Check effect of more rigorous algorithms - Case6 with Random Forest (RF) compared to Case2         | 3M_NC      | 35            | RF        | numTrees, maxDepth,                   | CV=5; 6-week rolling time folds | 6 2   | 7.1 hrs               |    F2 (lbl_0)=0.9616 <br> f1 score=0.7582 <br>areaUnderROC=0.6063      |\n",
    "|  6  | Check effect of more rigorous algorithms - Case7 with XGBoost (XGB) compared to Case2                    | 3M_NC      | 35            | XGB       | n_estimators,max_depth, learning_rate | CV=5; 6-week rolling time folds | 7 2   | N/A                |    xgb module not available    |\n",
    "|  7  | Check effects of NL features - Case8 with log terms, Case9 without NL terms and compare with Case2 (square terms) | 3M_NC      | 36 and 31            | LR        | regParam                              | CV=5; 6-week rolling time folds | 8 9 2 | 43.0 min 44.2 min |    Case8: <br> F2 (lbl_0)=0.9616 <br> f1 score=0.7582 <br> areaUnderROC=0.6648 <br> <br> Case9:<br>F2 (lbl_0)=0.9616 <br> f1 score=0.7582 <br> areaUnderROC=0.6648 |\n",
    "|  8  | Run best algorithm with all desired features (Case10; forming another basecase!)     | 3M_NC        | 34            | RF        | numTrees, maxDepth,                   | CV=5; 6-week rolling time folds      | 10 2  | 9.62 hrs              |  F2 (lbl_0)=0.93 to 0.96        |\n",
    "|  9  | Check effect of OHE of features with large numbe rof unique values on execution time (Case11) and compare with Cse10     | 3M_NC        | 32            | RF        | numTrees, maxDepth,                   | CV=5; 6-week rolling time folds    | 11 10  | 9.62 hrs              |   F2 (lbl_0)=0.87 to 0.96      |\n",
    "|  10  | Finally use desired conditions and run the model using the large 60M dataset (Case12)     | 60M        | 33            | RF        | numTrees, maxDepth,                   | CV=3; 1-year forward folds      | 12  | 2.45 hrs              |    F2 (lbl_0)=0.9562 <br> f1=0.7299 <br> areaUnderROC=0.5416       |\n",
    "|  11  | Apply downsample method to make delay and non-delay flights' data balance in each week (Case2b)     | 3M_NC        | 36            | LR        | \tregParam                   | CV=6; 6-week forward folds      | 2b 2  | 30 min              |  0.4852       |\n",
    "|  12  | Using 60M_NC Dataset & RF with smaller CVs (Case13)     | 60M_NC        | 33            | RF        | \tnumTrees, maxDepth                   | CV=13; 6-week forward folds      | 13  | 10.75 hrs              |    F2 (lbl_0)=0.9562 <br> f1=0.7299 <br> areaUnderROC=0.5416       |\n",
    "|  13  | Using 60M_NC Dataset & RF with smaller CVs and reducing feature space arbitrarily (Case13b)     | 60M_NC        | 10            | RF        | \tnumTrees, maxDepth                   | CV=13; 6-week forward folds      | 13  | 2.59 hrs              |    F2 (lbl_0)=0.9562 <br>f1 score=0.7299 <br>areaUnderROC=0.5       |\n",
    "|  14  | Using 60M_NC Dataset & NN(Case14)     | 60M_NC        | 33            | NN        | \tUDF to get vector size                   | CV=13; 6-week forward folds      | 13  | Unknown              |    Case14: refer to section \"Neural Network Implementation\"       |\n",
    "|  15  | Using 60M_NC Dataset & RF with smaller CVs + prior to 2-Hr Delay Feature(Case15)     | 60M_NC        | 34            | RF        | \tnumTrees, maxDepth                    | No CV, used best para from Case 6 due to the execution time issue     | 13  | 6 min              |    Weighted F2=0.7842 <br>F2_1 score: 0.0027 <br> F2_0 score: 0.9563  <br> f1=0.7308 <br> ROC_AUC=0.7014      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbbddf85-6830-4f7f-83a4-edb5520a0e07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below are the links to each of the experiment cases described above:\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836260284/command/3984215836260285\" target=\"_blank\">Case 1</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836263898/command/3984215836263949\" target=\"_blank\">Case 2</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632305377/command/4444148632305378\" \n",
    "target=\"_blank\">Case 2b</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836266291/command/3984215836266292\" target=\"_blank\">Case 3</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836266343/command/3984215836267160\" target=\"_blank\">Case 4</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836266845/command/3984215836266856\" target=\"_blank\">Case 5</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836266900/command/3984215836267168\" target=\"_blank\">Case 6</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836266952/command/3984215836267171\" target=\"_blank\">Case 7</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836267010/command/3984215836267021\" target=\"_blank\">Case 8</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836267061/command/3984215836267104\" target=\"_blank\">Case 9</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836268374/command/3984215836268417\" target=\"_blank\">Case 10</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836268432/command/3984215836268475\" target=\"_blank\">Case 11</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836268488/command/3984215836268489\" target=\"_blank\">Case 12</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836272436/command/3984215836272437\" target=\"_blank\">Case 13</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836273846/command/3984215836273847\">Case 13b</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/3984215836272811/command/3984215836277824\">Case 14</a>\n",
    "* <a href=\"https://adb-4248444930383559.19.azuredatabricks.net/?o=4248444930383559#notebook/4444148632303801/command/4444148632303802\">Case 15</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d470ef0-da7f-4e39-8b06-743456e3f43d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Final Model Results\n",
    "\n",
    "Based on the hyperparameter tuning results, it is identified that the final best model is the Random Forest model trained on 5 years (2015-2019) data, employing 50 trees with a maximum depth of 5. For the test set, the Random Forest model successfully predicts that 18% (4,318,951 instances) of flights will experience delays, while the remaining 82% (19,621,689 instances) will maintain their scheduled departure times. This class distribution indicates a certain level of class imbalance, with delayed flights being the minority class. The F1 score for the test set is calculated to be 0.731, suggesting that the model achieves a reasonable trade-off between minimizing false positives and false negatives. The weighted F2 score is 0.784, this indicates that the model's performance is strong in capturing the positive class (delayed flights) while also considering precision and recall trade-offs. \n",
    "\n",
    "Diving deeper, the F2 score for the positive label registers at 0.003, indicating that the model struggles to accurately identify delayed flights, whereas for the negative label, it is recorded at 0.956, indicating strong performance in correctly classifying on-time flights. The model achieves an accuracy of 0.814, meaning that the model correctly predicts the class of approximately 81.4% of instances. However, due to class imbalance, accuracy alone might not provide a complete picture of model performance. The precision and recall values are 0.848 and 0.814 respectively, indicating that when it predicts a flight will be delayed, it is correct around 84.8% of the time, and the model identifies around 81.4% of actual delayed flights. Additionally, the model's Receiver Operating Characteristic Area Under Curve (ROC AUC) stands at 0.701. A value of 0.701 suggests moderate discrimination ability, but there's room for improvement.\n",
    "\n",
    "| Dataset | Model Type | Hyperparameters |  F2 Score (Weighted) | F2 Score (Delay Class) | F2 Score (No-delay Class) | F1 Score |Accuracy | Precision | Recall | ROC-AUC |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| OTPW (2015-2019) | Random Forest | 50 trees, max tree depth 5 |  0.784 | 0.003 | 0.956 | 0.731 |0.814 | 0.848 | 0.814 | 0.701 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b21d51-e971-4658-ba82-d7d60d0c01ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Feature Importance\n",
    "\n",
    "\n",
    "To ascertain the most significant features, the chart below illustrates the feature importance derived from the final Random Forest model. Obviously, the feature importance values given by a tree decision estimator such as the ones below cannot be interpreted literlly. However, these values are relative, meaning that they can be compared against each other. A higher value indicates that a feature is more influential in making a decision than a feature with a lower value. From that perspective, the top-2 most predictive features are departure time and arrival time. It suggests that certain times of the day might be more prone to delays due to congestion, air traffic, or other operational factors. Delays at the arrival end can have a cascading effect on subsequent flights and airport operations.\n",
    "\n",
    "The third and fourth most important features are daily delay probability and seasonal delay probability. Different hours may experience varying levels of air traffic, weather conditions, or operational challenges.Different seasons can bring about diverse weather conditions and operational demands that affect flight punctuality. These two new features were created from feature engineering phase, where we observed a clear pattern between delays and hour of the day, as well as season of the year. The feature importance chart further confirms the hypothesis behind the strong relationship between departure delays and the time of day and the season of flying. [It should be noted that the importance of these two features might be influenced by the potential leakage dicsussed in the Data Leakage section earlier.]\n",
    "\n",
    "It is worth noting that the flight delay is more inlfuenced by `TAXI_OUT` than `TAXI_IN`, this is likely because delayed flights usually do not have `TAXI_OUT` value but only `TAXI_IN` value. This implies a flight will be likely delayed if it has missing value for `TAXI_OUT` feature. It is also found that the humidity and precipitation are more useful to predict flight delays than wind direction and sky conditions. We introduced a new `holiday` feature, however, it turns out there is no strong influence by the holidays on flight delay.\n",
    "\n",
    "Notes\n",
    "- `Delay_Daily_Prob` denotes the probability of a flight will be delayed for the given departure time. It is created in the \"Daily Trend\" section as above.\n",
    "- `Delay_Season_Prob` denotes the probability of a flight will be delayed for the given departure season. It is created in the \"Seasonality\" section as above.\n",
    "- `OP_CARRIER_AIRLINE_ID_ohe_20398` denotes if the flight's airline ID is 20398 or not, it is a one-hot encoded categorical feature based off of raw feature `OP_CARRIER_AIRLINE_ID`\n",
    "- `holiday_ohe_Thanksgiving Eve` denotes if the departure date is \"Thanksgiving Eve\" or not, it is one-hot encoded based off of the `holiday` feature\n",
    "| Feature Importance Chart | Feature Importance Table |\n",
    "| -- | -- |\n",
    "| <img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/Feature%20Importance.png?raw=true\" width=1500> | <img src=\"https://github.com/QianQiaoM/W261Team3-1/blob/main/Screenshot%202023-08-10%20at%209.35.42%20PM.png?raw=true\" width=300> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce1067d-2385-4f6e-be08-7862fde86624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Gap Analysis\n",
    "\n",
    "**Achievements and Strengths**\n",
    "\n",
    "In this project, several good practices were observed throughout the stages of exploratory data analysis (EDA), feature engineering, model training, and hyperparameter tuning. During the EDA step, duplicate rows and columns with a high percentage of missing values were handled, as well as columns with low variance, using rigorous data analysis techniques. Correlations between flight delays and time-based features such as hour of the day, season of the year, and holidays were identified.\n",
    "\n",
    "The feature engineering approaches improved the quality of features and enhanced predictive performance. Categorical ordinal features, including sky condition, wind direction, and airport type, were properly encoded to capture their influence on the model's variance. Quadratic and logarithmic transformations were applied to selected numerical features to account for their latent non-linear relationships. Additionally, new time-based features, such as holiday indicators and daily/seasonal distributions of flight delays, were created to further improve the model's ability to respond to time-based patterns. The new feature indicating whether a flight was delayed within two hours of scheduled departure is also included in the Random Forest model.\n",
    "\n",
    "Furthermore, the team conducted 15 experiments to evaluate model performance under various conditions, comparing the effects of introducing or removing specific features based on experimental results rather than speculation. The hyperparameter tuning process explored regularization parameters for the logistic regression model and the number of trees for the random forest model, leading to the identification of the best candidate as the final model. \n",
    "\n",
    "Finally, for the test set, the Random Forest model successfully anticipates that 18% (4,318,951 instances) of flights will experience delays, while the remaining 82% (19,621,689 instances) will maintain their scheduled departure times. The F1 score for the test set is calculated to be 0.731, while the weighted F2 score is 0.784. Diving deeper, the F2 score for the positive label registers at 0.003, whereas for the negative label, it is recorded at 0.956. The model achieves an accuracy of 0.814, with precision and recall values of 0.848 and 0.814, respectively. Additionally, the model's Receiver Operating Characteristic Area Under Curve (ROC AUC) stands at 0.701.\n",
    "\n",
    "**Areas for Enhancement**\n",
    "\n",
    "On the other hand, there are a few improvemnt areas for this project. The models are trained based off of data from 2015 to 2018 which predates the pandemic's impact. Consequently, there might be limitations in the generalizability of the trained model since it doesn't contain data obtained from 2019 to 2021. We hypothesize that flight delay is mainly caused by weather conditions, however, the weather condition data is only available for departure airport but not for arrival airport.\n",
    "\n",
    "In the EDA step, some uesful features may be dropped due to high (90%) percentage of missing values or based on common transporation and aviation knowledge. For the cancelled flight, some of them may be cancelled due to long delay. It is unclear how much of the cacelled flights are due to long delay. Flights getting cancelled due to long delay will be helpful to train the model, we explored two scenarios, one of which is treating all cancelled flights as delayed flgihts, and the other is treating them as non-delayed flights. This could introduce bias to the model and lower the model performance.\n",
    "\n",
    "In the feature engineering step, the qudratic function and logarithm function are applied to a few existing columns. This only makes sense when there is a qudratic or logarithmic relationship between the features and label. Categorical features that are non-ordinal are encoded by OneHotEncoder, which in some case has extensive number of distinct values, significantly increasing the computational complexity of the model training process. An alternative approach such as target encoding could be considered to alleviate this issue. The graph feature added in this step is defined as the degree centrality weighted by daily throughput. An alternative graph based feature is to extract the degree centrality based on the connections between any two airports, or using the PageRank algorithm to compute the importance of a specific airport for the entire airports graph.\n",
    "\n",
    "In the modeling step, the train and validation sets are split with a per-year grain, however, there are only four years data used as training and one year used for testing. The latent pattern embodied in the three year training data may not have enough samples to caputre the entire variance for specific days. Finally, the model's performance optimization is based on a limited number of hyperparameters, which does not guarantee the best performance for the given training data. A more comprehensive hyperparameter search could be conducted to potentially improve model performance further.\n",
    "\n",
    "Besides, we attempted to figure out the slow training time with PySpark, the team tried several techniques including caching intermediate results, optimizing partition, and parallizing spark jobs. However, it still takes more than 10 hours to train model with 60 month data. A more comprehensive investigation is desired to improve the model training time with large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef46b0b0-8ae5-45ac-babc-91dcc0d802f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "This project investigates the feasibility of predicting a departure delay of 15 minutes for flights by leveraging historical flight and weather data. It involves the evaluation of various machine learning models and features to make accurate predictions of flight departure delays, enabling passengers to plan better and benefiting the aviation industry by minimizing operational costs and improving passenger satisfaction. \n",
    "\n",
    "**Final Results**\n",
    "\n",
    "The Random Forest model trained on 5 years (2015-2019) data, employing 50 trees with a maximum depth of 5, demonstrates the best performance compared to other models. The weighted F2 score is 0.784. The remaining scores are provided in the table below. \n",
    "\n",
    "| Dataset | Model Type | Hyperparameters | F1 Score | F2 Score (Overall) | F2 Score (Delay Class) | F2 Score (No-delay Class) | Accuracy | Precision | Recall | ROC-AUC |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| OTPW (2015-2019) | Random Forest | 50 trees, max tree depth 5 | 0.731 | 0.784 | 0.003 | 0.956 | 0.814 | 0.848 | 0.814 | 0.701 |\n",
    "\n",
    "The model shows promise in predicting on-time flights, as reflected by its high F2 score for the negative label and overall accuracy.\n",
    "However, the model's performance in identifying delayed flights (positive label) is poor, as indicated by the extremely low F2 score for this label.\n",
    "There is a class imbalance issue that might need to be addressed to improve the model's predictive power for delayed flights.\n",
    "Improving recall for the positive class is crucial for an airline's operations, as it helps prevent passenger inconvenience and improve customer satisfaction.\n",
    "Further analysis and model refinement are needed to enhance the model's ability to accurately predict and identify delayed flights.\n",
    "\n",
    "\n",
    "Per feature importance analysis, the top-10 deciding factors (sorted by importance in descending order) of flight delays are:\n",
    " - 1. departure time\n",
    " - 2. arrival time\n",
    " - 3. hour of the day \n",
    " - 4. season of the year\n",
    " - 5. hourly humidity\n",
    " - 6. taxiing out duration\n",
    " - 7. hourly precipitation\n",
    " - 8. hourly temperature\n",
    " - 9. sea level pressure\n",
    " - 10. distance\n",
    "\n",
    " Overall, these top-10 factors collectively demonstrate that flight delays are influenced by a combination of operational, weather-related, and timing considerations. The analysis implies that airlines and airports should pay close attention to these factors and consider implementing strategies to mitigate delays, such as optimizing scheduling, contingency planning for adverse weather conditions, and improving taxiing procedures to reduce ground time. The insights from this analysis could assist in making informed decisions to enhance flight punctuality and passenger satisfaction.\n",
    "\n",
    "**Exploratory Data Analysis (EDA)**\n",
    "\n",
    "In the EDA process, each dataset's metadata including file size, number of rows/columns, table summary and table schema were explored. Besides, the missing values and duplicate rows are identified with various strategies including percentage of missing values, number of distinct values in the column, and variance of the column. These strategies were applied to the joined dataset to reduce the dimensionality of features and keep the model lean and training process efficient. Specfically, columns with close to 100% missing values, columns with extreme low variance, and duplicate rows were dropped. Besides, we also found the flight delay pattern based on holiday information, hour of the day, and season of the year. The insights of seasonality and trend inspired us to create new time-based features in the later step. Understanding data intricacies significantly improved the data quality and benefit model development.\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "The feature engineering approaches improved the quality of features and enhanced predictive performance. Categorical features, including sky condition, wind direction, and airport type, were properly encoded to capture their influence on the model's variance. Quadratic and logarithmic transformations were applied to selected numerical features to account for their latent non-linear relationships in the logistic regresson algorithm. Additionally, new time-based features, such as holiday indicators and daily/seasonal distributions of flight delays, were created to further improve the model's ability to respond to time-based patterns. In the funneling process we also applied suitable transformations to simplify index encoding and maximizing the effectiveness of these features in our model. We also introduced new features to reflect sesonality including a binary \"holiday\" feature as well as numerical probabilty features. Finally, we also added a graph-based feature (\"Outdegree\") to capture essential relationships in the data and improve the model's predictive capabilities. The feature set contains 47 features for model training, with 32 numerical features and 15 categorical features.\n",
    "\n",
    "**Modeling and Experimentation**\n",
    "\n",
    "In the modeling experiment, there are 15 experiments cases compared including three machine learning models: Logistic Regression, Random Forest, and XGBoost. The goal of these expeirments are designed to explore the effect of cancelled flight, holidays, seasonality, model complexity, non-lenearity for numerical features, and cardinality for categorical features. Optimal hyperparameters are determined using F2 beta metrics, while F1, accuracy, precision, and recall are reported for models with the best hyperparameter settings. These experiments are crucial in fine-tuning and selecting the most effective models for accurate flight delay predictions. \n",
    "\n",
    "The final best model is the Random Forest model trained on 5 years (2015-2019) data, employing 50 trees with a maximum depth of 5. For the test set, the Random Forest model successfully predicts that 18% (4,318,951 instances) of flights will experience delays, while the remaining 82% (19,621,689 instances) will maintain their scheduled departure times. The F1 score for the test set is calculated to be 0.731, while the weighted F2 score is 0.784. Diving deeper, the F2 score for the positive label registers at 0.003, whereas for the negative label, it is recorded at 0.956. The model achieves an accuracy of 0.814, with precision and recall values of 0.848 and 0.814, respectively.\n",
    "\n",
    "**Future Work**\n",
    "\n",
    "To further enhance our model's performance, we recommend exploring more predictive features including weather for the arrival airport. Moreover, the incorporation of feature crosses, or interaction terms, can potentially provide nuanced insights into the data. Considering the potential for capturing intricate patterns within the dataset, the utilization of advanced models such as Support Vector Machines and Deep Neural Networks is recommended. Furthermore, increasing the training dataset size could enhance generalization capabilities and overall model performance. Finally, optimizing the efficiency of PySpark code is crucial to expedite model training procedures, particularly when dealing with substantial datasets."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3984215836259006,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Team3-1_W261_Final_Project_Phase_Four_Report",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
